{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pandas.read_csv()\n",
    "##The pandas python library provides read_csv() function to import CSV as a dataframe structure \n",
    "##to compute or analyze it easily. This function provides one parameter described in a later \n",
    "##section to import your gigantic file much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e99c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5873a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay_bijalwan.PATANJALI\\AppData\\Local\\Temp\\ipykernel_6680\\2396633188.py:2: DtypeWarning: Columns (5,7,31,32,34,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  patient_2016 = pd.read_csv(r'patient_2017.csv', encoding='latin1')\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "patient_2016 = pd.read_csv(r'patient_2017.csv', encoding='latin1')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992bebf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read csv without chunks:  124.60359001159668 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Read csv without chunks: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pandas.read_csv(chunksize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db92a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Instead of reading the whole CSV at once, chunks of CSV are read into memory.\n",
    "##The size of a chunk is specified using chunksize parameter which refers to the number of lines.\n",
    "##This function returns an iterator to iterate through these chunks and then wishfully processes them. \n",
    "##Since only a part of a large file is read at once, low memory is enough to fit the data. Later, \n",
    "##these chunks can be concatenated in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1abb3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_1 = time.time()\n",
    "#read data in chunks of 1 million rows at a time\n",
    "chunk = pd.read_csv(r'patient_2017.csv',encoding='latin1', chunksize=1000000)\n",
    "end_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9e5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read csv with chunks:  0.9505438804626465 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Read csv with chunks: \",(end_1-start_1),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0f758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
